{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aeecccad",
   "metadata": {
    "height": 404
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting tools for paper: metagpt.pdf\n",
      "Getting tools for paper: longlora.pdf\n",
      "Getting tools for paper: selfrag.pdf\n"
     ]
    }
   ],
   "source": [
    "urls = [\n",
    "    \"https://openreview.net/pdf?id=VtmBAGCN7o\",\n",
    "    \"https://openreview.net/pdf?id=6PmJoRfdaK\",\n",
    "    \"https://openreview.net/pdf?id=hSyW5go0v8\",\n",
    "]\n",
    "\n",
    "papers = [\n",
    "    \"metagpt.pdf\",\n",
    "    \"longlora.pdf\",\n",
    "    \"selfrag.pdf\",\n",
    "]\n",
    "\n",
    "from utils import get_doc_tools\n",
    "from pathlib import Path\n",
    "\n",
    "paper_to_tools_dict = {}\n",
    "for paper in papers:\n",
    "    print(f\"Getting tools for paper: {paper}\")\n",
    "    vector_tool, summary_tool = get_doc_tools(paper, Path(paper).stem)\n",
    "    paper_to_tools_dict[paper] = [vector_tool, summary_tool]\n",
    "\n",
    "initial_tools = [t for paper in papers for t in paper_to_tools_dict[paper]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "659a7271",
   "metadata": {
    "height": 47
   },
   "outputs": [],
   "source": [
    "from llama_index.llms.openai import OpenAI\n",
    "llm = OpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64e1b67e",
   "metadata": {
    "height": 64
   },
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgentWorker\n",
    "from llama_index.core.agent import AgentRunner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "873b22c6",
   "metadata": {
    "height": 115
   },
   "outputs": [],
   "source": [
    "agent_worker = FunctionCallingAgentWorker.from_tools(\n",
    "    initial_tools, \n",
    "    llm=llm, \n",
    "    verbose=True\n",
    ")\n",
    "agent = AgentRunner(agent_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5369ab7b",
   "metadata": {
    "height": 81
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in LongLoRA, and then tell me about the evaluation results\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation dataset\"}\n",
      "=== Function Output ===\n",
      "PG19 test split\n",
      "=== Calling Function ===\n",
      "Calling function: vector_tool_longlora with args: {\"query\": \"evaluation results\"}\n",
      "=== Function Output ===\n",
      "The evaluation results show that the models achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. Additionally, the models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results even on extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n",
      "=== LLM Response ===\n",
      "The evaluation dataset used in LongLoRA is the PG19 test split. \n",
      "\n",
      "Regarding the evaluation results, the models in LongLoRA achieve better perplexity with longer context sizes. Increasing the context window size leads to improved perplexity values. The models are fine-tuned on different context lengths, such as 100k, 65536, and 32768, and achieve promising results even on extremely large settings. However, there is some perplexity degradation observed on small context sizes for the extended models, which is a known limitation of Position Interpolation.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used in LongLoRA, \"\n",
    "    \"and then tell me about the evaluation results\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "273b010d",
   "metadata": {
    "height": 64
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Give me a summary of both Self-RAG and LongLoRA\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/llama_index/core/tools/calling.py:22: RuntimeWarning: coroutine 'run_async_tasks.<locals>._gather' was never awaited\n",
      "  return ToolOutput(\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/usr/local/lib/python3.11/site-packages/llama_index/core/tools/calling.py:22: RuntimeWarning: coroutine 'Dispatcher.span.<locals>.async_wrapper' was never awaited\n",
      "  return ToolOutput(\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== LLM Response ===\n",
      "I encountered an error while trying to generate summaries for Self-RAG and LongLoRA. Let me try again.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG is a self-supervised method for generating abstractive summaries of long documents. It uses a retrieval-augmented generator to improve the quality of generated summaries.\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA is a long-range language model that can generate coherent and informative long-form text. It uses a hierarchical architecture to capture dependencies across multiple scales and generate high-quality text.\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== LLM Response ===\n",
      "I apologize for the inconvenience. It seems there is still an issue with generating summaries for Self-RAG and LongLoRA. Let me try to resolve this.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"Self-RAG is a self-supervised method for generating abstractive summaries of long documents. It uses a retrieval-augmented generator to improve the quality of generated summaries.\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_longlora with args: {\"input\": \"LongLoRA is a long-range language model that can generate coherent and informative long-form text. It uses a hierarchical architecture to capture dependencies across multiple scales and generate high-quality text.\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== LLM Response ===\n",
      "I apologize for the continued issue. It seems there is still a problem with generating summaries for Self-RAG and LongLoRA. Let me try one more time to resolve this.\n",
      "assistant: I apologize for the continued issue. It seems there is still a problem with generating summaries for Self-RAG and LongLoRA. Let me try one more time to resolve this.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\"Give me a summary of both Self-RAG and LongLoRA\")\n",
    "print(str(response))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c600a139",
   "metadata": {
    "height": 98
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added user message to memory: Tell me about the evaluation dataset used in MetaGPT and compare it against SWE-Bench\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== LLM Response ===\n",
      "I encountered an error while trying to retrieve the information about the evaluation datasets used in MetaGPT and SWE-Bench. Let me try to fetch the information again.\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_metagpt with args: {\"input\": \"evaluation dataset used in MetaGPT\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== Calling Function ===\n",
      "Calling function: summary_tool_selfrag with args: {\"input\": \"evaluation dataset used in SWE-Bench\"}\n",
      "=== Function Output ===\n",
      "Encountered error: asyncio.run() cannot be called from a running event loop\n",
      "=== LLM Response ===\n",
      "I apologize for the inconvenience. It seems there is an issue with fetching the information about the evaluation datasets used in MetaGPT and SWE-Bench. Let me try to provide a general comparison based on what I know.\n",
      "\n",
      "The evaluation dataset used in MetaGPT is designed specifically for evaluating the performance of the MetaGPT model. It may consist of various text data sources, prompts, and evaluation metrics tailored to assess the model's language generation capabilities.\n",
      "\n",
      "On the other hand, SWE-Bench is a benchmark dataset designed for evaluating software engineering tasks. It contains a collection of software engineering tasks, code snippets, and evaluation criteria to assess the performance of models in software engineering-related tasks.\n",
      "\n",
      "In summary, the evaluation dataset used in MetaGPT is likely focused on language generation tasks, while SWE-Bench is tailored for evaluating software engineering tasks. The specific details and characteristics of each dataset would provide a more comprehensive comparison.\n",
      "assistant: I apologize for the inconvenience. It seems there is an issue with fetching the information about the evaluation datasets used in MetaGPT and SWE-Bench. Let me try to provide a general comparison based on what I know.\n",
      "\n",
      "The evaluation dataset used in MetaGPT is designed specifically for evaluating the performance of the MetaGPT model. It may consist of various text data sources, prompts, and evaluation metrics tailored to assess the model's language generation capabilities.\n",
      "\n",
      "On the other hand, SWE-Bench is a benchmark dataset designed for evaluating software engineering tasks. It contains a collection of software engineering tasks, code snippets, and evaluation criteria to assess the performance of models in software engineering-related tasks.\n",
      "\n",
      "In summary, the evaluation dataset used in MetaGPT is likely focused on language generation tasks, while SWE-Bench is tailored for evaluating software engineering tasks. The specific details and characteristics of each dataset would provide a more comprehensive comparison.\n"
     ]
    }
   ],
   "source": [
    "response = agent.query(\n",
    "    \"Tell me about the evaluation dataset used \"\n",
    "    \"in MetaGPT and compare it against SWE-Bench\"\n",
    ")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d96939",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
